{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "#When dealing with high-dimensional inputs such as images, \n",
    "#it is impractical to connect neurons to all neurons in the previous volume. \n",
    "#Instead, we will connect each neuron to only a local region of the input volume. \n",
    "#The spatial extent of this connectivity is a hyperparameter called the receptive field \n",
    "#of the neuron (equivalently this is the filter size). \n",
    "#smaller size than input\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "#more filters, featuer map will b\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Test-set:\t\t10000\n",
      "- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(data.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.validation.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test.cls = np.argmax(data.test.labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "#https://en.wikipedia.org/wiki/Channel_(digital_image)\n",
    "#channels mean number of primary colors\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = data.test.images[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = data.test.cls[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_biases(length):\n",
    "    #equivalent to y intercept\n",
    "    #constant value carried over across matrix math\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-3d15d90b0525>:4: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 7, 7, 36) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?, 1764) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1764"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_3:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-2dd067a7547b>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch = data.train.next_batch(train_batch_size)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example_errors(cls_pred, correct):\n",
    "    # This function is called from print_test_accuracy() below.\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # correct is a boolean array whether the predicted class\n",
    "    # is equal to the true class for each image in the test-set.\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = data.test.images[incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = data.test.cls[incorrect]\n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cls_pred):\n",
    "    # This is called from print_test_accuracy() below.\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = data.test.cls\n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.matshow(cm)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test-set into smaller batches of this size.\n",
    "test_batch_size = 256\n",
    "\n",
    "def print_test_accuracy(show_example_errors=False,\n",
    "                        show_confusion_matrix=False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.test.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.test.images[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.test.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images,\n",
    "                     y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Convenience variable for the true class-numbers of the test-set.\n",
    "    cls_true = data.test.cls\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "\n",
    "    # Plot the confusion matrix, if desired.\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 11.2% (1121 / 10000)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      1, Training Accuracy:  12.5%\n",
      "Time usage: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time usage: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=99) # We already performed 1 iteration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 69.1% (6908 / 10000)\n",
      "Example errors:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD5CAYAAACj3GcTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcFNW5//HPo6gguCFGuSqMP0HAJaJBwF2DW8QraghoRKPGLXpJcIlX43LdkhiNgMaooPcmGo0aUXHBDdySKIogiIi4RhQSFFRUjILC+f3R9XRVz9ZdM70O3/frxaurq09Vn+FMnXnq1FkshICIiBRmjUpnQESklqjSFBFJQZWmiEgKqjRFRFJQpSkikoIqTRGRFFRpioikoEpTRCQFVZoiIim0a83BXbp0CXV1dUXKSm2YMWPGkhDCJpXOR7mojNs+lXE6rao06+rqmD59emtOUXPMbH6l81BOKuO2T2Wcjm7PRURSUKUpIpKCKk0RkRRUaYqIpKBKU0QkBVWaIiIpqNIUEUmhVf00RSrhk08+AeC9995rMk337t0BGDNmDADbb789ANtssw0AO+64YymzKG2YIk0RkRQUaUrVe+ihhwB48MEHAXj66acBePPNN5s8plevXgC8++67ACxfvjzn81WrVhU5l7K6UKQpIpJCVUean332GQDnnnsuAK+++ioAU6ZMyaZZa621yp8xKbq3334bgN///vcAjB8/PvvZl19+CUCa5aZff/31IuZOJKZIU0QkhaqMNG+77TYALrjgAqDhU1KPQAE23njj8mVMSmbBggUAjB07tlXn6d27NxA/LZfq89ZbbwGwZMmS7L777rsPiNur11gjE8+deuqpAOy2227ZtD179ixHNpukSFNEJIWqijQ92jjjjDOA+C+RmeWkGzlyZHb7uuuuA6Bz587lyKK0QDKi8Ehyjz32AOCggw4CYO211wZggw02AKBTp07ZY5YtWwbAgQceCMRR5IABAwDYaaedsmk7dOgAQMeOHYv8U0hLvfLKK0DcXn3vvfcCsHjx4rzHPv/880DuswvvGeG/Q9dccw0Q/w6VmiJNEZEUVGmKiKRQVbfnv/3tbwH46KOPmk135513ZrcfeeQRIH5o5Lfu5QrVpWlffPEFAPvvv39238svvwzAxIkTc9LuuuuuAMycORPILMHg/EHgFltsAcQPCaQ6zZ49G4hvx++66y4APv3005x0Xp4Ae+65JxCX+1VXXQXAd77zHQBeeOGFbFqvHx5++GEgHhLrD41KTb99IiIpVDzSnD8/Xt/oD3/4Q85n/hdk0003BWDy5MkNjve/Xh6lHn300QBsttlmxc+sFGTFihUA/PCHPwTi6BLgF7/4BQD77bdfo8c2tipit27dipxDKbZTTjklu+3dh+o/6PEy32GHHQD41a9+lf2sffv2OWmnTp0KwA033ADA8ccfn/1s1qxZQHyNn3baaQB8//vfB2CTTUq7kKgiTRGRFCoeafpfDYg7re+1114APPPMMwB89dVXAPz5z38G4Ne//nX2GO8ou2jRIgCGDBkCxG2d6opUPt41yCMIn2Aj+Zf/5z//OQDrrrtumXMnxeTX5JVXXgnATTfdlP3Mh7t+61vfAuAnP/kJEJd9Id3BvN3ym2++AeCSSy7JfuZdz3wylnJTpCkikkLFI83klF3eid07tztv7zjhhBMAmDBhQvYzn+jB/7p5BKOn5+XnT8SvuOIKIJ4I+G9/+1s2jXdel9rmwx39KXdyMpXNN98ciDux9+/fP+/5Vq5cCcD7778PwLHHHgvA4MGDgXji6cYcc8wxAGy44YYF5781FGmKiKRQ8UjzjjvuaLBv0qRJABx22GGNHjN9+vQmzzdw4EAgdxielMdzzz2X896HNyb740nb4G2Na665ZoPPfMij9630O8N58+blpPMhrwCvvfZazmuXLl2A+FlFY7xXjffRLtc0kYo0RURSqHikedRRR2W377//fgBefPFFIP7L5AP+vf9Xsn3D2zF8n09e6+0c2267bcnyLrmSbc0Q92BIPvk89NBDgdxJNqT2DBo0CIB9990XyO1D7X2vf/rTnzZ6bLt2mWrHo9XG1I8wk6PAjjjiCACuvfZaALp27Zoq762lSFNEJAVVmiIiKViadVfq69evX2juoUwhPv744+z21ltvDcRDIz1v9efTTE4A4ZMCHHLIIQC88cYbAJx88skA3Hjjja3KX31mNiOE0K+oJ61iacrYy6l+eSX5gwOfXMHnxPSuJj169ABgu+22a3CsrxHlk3uU6gGTyji9pUuXZre9y9mzzz4LxKsr+HBY72aYHF6bnJCjMd5BHuLBE63pYtSaMlakKSKSQsUfBCWHOd59990ADB06FGgYcXrD8m9+85vsMd7x3RuHfYjlY489BsSd3yGOZKU0zj77bACuvvrqJtN4J2a/Q/DXNHx43j777APkThUolZGM+jzSzMc7sEPDSHP99dcHYPTo0QAcd9xx2c8a6+ZUToo0RURSqHikmeRTR3nXFZ+gw/+KXXrppUDDaaQALrzwQiDuHOvdl/wYgFtuuaUU2ZaIRxjDhg0D4mn6vv7662waXwfKI86W+PDDD4H4ziS58qR3dJbq5ZN8NHeH4FPC+fSC1USRpohIClUVaTqPOJuaqLYxPiRr+PDhQBxpPvXUU9k0/qRe08WVhrc17bLLLkDckyHpiSeeAOLo8+KLLwZg2rRpqb/P27pnzJiR+lgpv5tvvhmAyy+/HMi9A3F+1+ATClcjRZoiIilUZaTZGt6e9sADDwC57Sa+RvpFF11U/owJEA+/cz4JtUeaPulCcnmDk046CYAxY8YAcVu31AYv27POOguAzz//vEGa9dZbD4jbMtdZZ50y5S49RZoiIimo0hQRSaHN3Z77bCjnnHMOkLu+tj90OPLIIwHYZpttyps5aeCAAw4A4lUq/eGAz1YF8OabbwLxbOH1+UzhUp18rShfA8wl1wry5rQ99tijfBlrIUWaIiIptLlI0/Xt2xeAyy67LLvPh/mdd955ANx2221A7gzSUl59+vQB4q5id911V4M0yW5jEM/H6OvHJIfVSvXwBz7emb2+ESNGZLd9SGwtUKQpIpJCm400XXJSgHHjxgHxKnneVvbtb3+7/BkTII7yx44dC8TRSbLD+gcffABAXV0dEJept1FLdVm2bBkQ30WsWLEi5/Mdd9wRiMu81ijSFBFJoc1Hmptsskl2e8qUKUC8HrdPMKHO0pXnKws+9NBDAPzpT3/KfjZ16lQgjix9ajipTk8++SQACxcubPRzn+6tsYl3aoEiTRGRFNp8pJnk0+37chneN2zu3LmAVq6sJr6aaP1tqX4+TWN93nf6u9/9bjmzU3SKNEVEUlitIk3nkxz7U7y33noLUKQpUgzJxRIhboMeNWpUJbJTdIo0RURSUKUpIpLCanl77ivd/eMf/6hwTkTanjPPPDPn1R8Mde3atWJ5KiZFmiIiKayWkaaIlM4ZZ5yR89rWKNIUEUnBfEW/Fh1sthiYX7zs1ITuIYRN8idrG1TGbZ/KOJ1WVZoiIqsb3Z6LiKSgSlNEJIVmK00z29jMZkX/FpnZwsT7tUuVKTM708xejf6NLCD9iWa2OMrXa2Z2Qiu//zYzO6zAtLua2cpC01ebCpbxAjN7JfqeFwpIrzJuIV3HBaUtuIyb7XIUQvgI6Bud9GJgWQjht/W+zMi0ja4qJHP5mFlf4EdAP+Ab4HEzeyiEkK8n+u0hhFFmthkwx8weCCEsSZy3XQjhm2LkMXlO4FfA5GKet5wqUcYJe4YQlqZIrzJuAV3HefOaqoxbdHtuZj3MbK6Z3Q68CmxpZksTnx9pZjdH25ua2b1mNt3MppnZwDyn7wM8H0L4MoTwNfBX4PBC8xZCWAS8C3Qzs8vN7FYzexb4o5m1M7PRUT5mm9mJUR7XMLPrzWyemU0GuhT4daOAO4El+RLWmhKXcauojItD13FWqjJuTZtmb2BMCGFboPEpmjOuBa4MIfQDhgFeCAPM7MZG0r8C7G1mnc2sI/A9YMtCM2VmPYDuwDuJfA4KIYwATgY+DCH0B3YBTjezbsBQYCtgW+B4YLfE+X5pZgc38j3dgMHATYXmrQaVqowBAvCkmc0wsx+nyZTKuKh0Hacs49aMCHo7hDC9gHT7Ab0y0T8AG5lZhxDCC0CDtqwQwhwzGw1MAZYBM4GVBXzP0Wa2D7AcODGEsDT6zvtDCF9FaQ4A+pjZkdH7DYCewF7AHdGtyQIzezqRn/Ob+L6xwDkhhFWJn62tKUkZRwaGEBZGt2GTzey1EMJzeb5HZVx8uo5TlnFrKs0vEturgOQ3Jhf/MKB/CCF3SbpmhBDGA+MBzOxK4K0CDrs9hNDYhH3JfBpwWgjhiWQCMyv4tiGhH3B39B/dBTjAzFaGEB5swbmqVSnLeGH0usjM7gf6A/kqTZVx8ek6TlnGRelyFNXsn5hZTzNbg9y2iynA6f7GMg3EzTKzb0WvdcChZNobMLOfmdmprcjqY8Bplmn4xcx6mVkHMu0tw6M2kc2BvfOdKITQLYRQF0KoAyYCJ7exiylHMcvYzDqZWadouyOwPzAneq8yrhBdx4WVcTH7af43mR/mOWBBYv/pwO5Rg+1c4CTI2941MUo7ETg1hPBZtL8P8FEr8jgOeBOYZWZzgBvIRNsTgPeAucAfgKl+QFNtIaupYpVxV+BZM3sZmAbcF0KYEn2mMq4sXcd51NQwSjObBAwpdpcDqR4q47av1su4pipNEZFK0zBKEZEUVGmKiKSgSlNEJAVVmiIiKbRqjaAuXbqEurq6ImWlNsyYMWPJ6jSrt8q47VMZp9OqSrOuro7p0wsZgdV2mNlqtSyAyrjtUxmno9tzEZEUVGmKiKSgSlNEJAVVmiIiKajSFBFJQZWmiEgKrepyJFIsM2bMAOC+++4D4J577sl+9vrrrwPgk8v4DNvf+c53AOjTp0827Xnnnddgn0gxKdIUEUlBkaaU3Pjx47Pb8+bNA+Bvf/tbThqPND2KTE5Z6PtOOeUUAA4/PDOh+AEHHFCiHIs0TZGmiEgKijSl5DxChDhqXHfddYG47XHUqMxaWr179wagS5d4yeojjjiiLPmU0nj66acBuPfeewGYMGECAP/617+yaXbaaScAhg0bBsC5555bxhymo0hTRCSFmog0Z86cCcCFF14IwMMPP5z9rP4T1R/84AcA/PKXvwSga9eu2bRPPfUUAIMGDQKgQ4cOpcy2RJKR4sSJE4E4wnzxxRcrkicpnUWLFgFx2/O0adOA+FrdcsstAejVq1f2mPfffx+A88/PLE/evXt3AI466qgy5DgdRZoiIilUZaT59ddfA/DMM88AcNxxxwFxG4hHlUm+z9tLPIp87733smm8beXWW28FYMSIEUXOuTTmxhvjFV5feuklAObPz8zM5eXTrVu38mdMimbJkiXZ7YMPzqyUO2vWLCCOGseNGwfAgAEDANhggw2yx3ikeeihhwJw9913AzB8+PCc9xC3f/bs2RNovD4oJUWaIiIpqNIUEUmhKm/P/RbuwAMPzNn/H//xHwBcd9112X3edcX5bZ/vHzlyZPazddZZB8h9OCSlt8km8aoCJ510EgAXXHABEN/W6fa8tl111VXZbb8t33zzzYF4GOzaa6/d5PH+cMib1/xa9Ye+jT0Q+uKLL4DyP9BVpCkikkJVRZpz5swB4sZgt99++wHw61//GoCdd965yXP885//BGDIkCEALF26NPvZOeecA8RdjqT8Vq1aBcTdT+bOnZvzvjHePan+XYVU3p133gnA6NGjs/s23nhjAF577TWg+Qizvq233hqIfy+OOeaYBmkOO+wwANq3b9+CHLeeIk0RkRSqKtK8/PLLAVi8eDEAhxxyCABXX301EHcxaI5Hq94umnTQQQcVJZ+SjpcnwP/+7/8CcTeRH/3oR0DDQQrJyNM7SR999NGAhlVWk9mzZwOwcuXK7L7tttsOgE6dOrX4vFtssUWTn6233npA+bsaOUWaIiIpVDzS9KepAH/5y1+A+C/UFVdcARQWYXqHeG/39Ehln332yabZe++9W59hKZhHmHvttVd2n/duqD+B8B577JFz7E033ZTd9rsGn/DBIwwfgpmccFjtnuX19ttvN9jnzw5a47HHHgPgq6++avCZD5WuFEWaIiIpVDzSnD59enbbI4iOHTsCsO222+Y93iNMn8zjr3/9a865LrroouJlVlLxCYe9nx7A97//fSB3WFxjTj755Oy29+W87bbbgHjSj1122QXI/T3x82q5i9L697//DcTLkyR5/8yWWLFiBQC/+MUvAFi+fDkQt2MC7LDDDi0+fzEo0hQRSaHikWZLvPvuu9nt66+/HoifsDsfPdS3b9+y5Uty7bnnnkDcN7OlfEJin6jYX30ZjWT7p7dbP/LII0Dcdiql8c033xTlPH7H+OSTTwIN20pPOOGE7LZPAFIpijRFRFJQpSkikkLFb8+TDfbeUfbjjz8G4nnz6kt2lvZhk/U7uvpQyQ033LB4mZWq4g+Lkp3dvXvT4MGDgbj5Rh3ii6tdu0zVUVdXB+Q2mT3++OMA7Ljjjs2eI7lG0J/+9Ceg6bWBfE7daqBIU0QkhYpHmj6sDuDzzz8HYNKkSUAceTbngQceAOK/VD611KmnnlrUfEr1Sq5c6bPEn3XWWUD8e+AzxPtDJGkdn4TDu/glu31553aPOL2bmU/C4de5HwvwwQcfAPFs7j7Rjj/08anjqoEiTRGRFCoeaSYnEH3wwQeBeC2fZMd3iP+a+RokAKeddhoQd2r2Fe58iilZvXibpnc58vceeSrSLC6fWMMHHkC8EuwTTzyR8+rR6VZbbQXkDnH+4Q9/CMST9Pgziu9+97sAdO7cuST5bwlFmiIiKVQ80myM/wVK/iVqirdh+V8mH1qXXGJBVj/ezukd7H1Ip5RGcuLw733vewDMmDEjJ41Hmo1NIv7GG28A8bBJN3To0KLmsxgUaYqIpFCVkWY+yT5hzgf0q81KIF5qwSf3KGTyFymOtdZaC4CBAwcWfMyCBQsa3Z/mHOWiSFNEJAVVmiIiKdTk7fmll17aYJ93VWhupUqpjDFjxmS3/QHdiBEjSvJdPjP8+eefD8RrYz/zzDMl+T4pDh+UUgsUaYqIpFBTkaavNOlrxSRppcnq4+XkHcsBTjnlFKBlkaZP1FJ/tvDke19PyCNaH17bu3fv1N8npeVDWwHuuOOOnM98XtT111+/rHkqhCJNEZEUairSnDlzJgCfffZZdp93am/fvn1F8iT5JdcwHzduHAD33HMPEE/Z5mm8E/rGG2+cPca7DTW1NnpyekFfG93XmElO5iHV5a233spuf/rppzmfDRkyBIinoKsmijRFRFKovmq8Gd6mlZxwePvttweqc7jV6s6jyEcffTS7z6NG5+2RH374IRB3Qk+WsbeDetR4+OGH55wj2V6pdc9rR3IyceflN3LkyHJnp2CKNEVEUqipSNOfhCYdc8wxFciJpHHggQc2ug1www03lDs7UiW8XTvJ1zRfc801y52dginSFBFJoaYiTX9KWsgyGCJS3XzicIjbsJtaTLGaKNIUEUlBlaaISAo1dXvuM0K/88472X0+U7uI1JbkoIdaokhTRCSFmoo0vXuRuhmJSKUo0hQRScFa065gZouB+cXLTk3oHkJYbZa6VBm3fSrjdFpVaYqIrG50ey4ikoIqTRGRFFRpioik0GylaWYbm9ms6N8iM1uYeL92qTJlZmea2avRv7wT65nZiWa2OMrXa2Z2Qiu//zYzO6zAtLua2cpC01ebCpbxAjN7JfqeFwpIX/YyNrMjzGx29J0vmtlurfnOStF1XFDagq/jZvtphhA+AvpGJ70YWBZC+G29LzMyD5RWFZK5fMysL/AjoB/wDfC4mT0UQvhHnkNvDyGMMrPNgDlm9kAIYUnivO1CCN8UI4/JcwK/AiYX87zlVIkyTtgzhLA0Rfpyl/HjwH0hhGBmOwO3AtsX8fxloes4b15TXcctuj03sx5mNtfMbgdeBbY0s6WJz480s5uj7U3N7F4zm25m08xsYJ7T9wGeDyF8GUL4GvgrcHieY7JCCIuAd4FuZna5md1qZs8CfzSzdmY2OsrHbDM7McrjGmZ2vZnNM7PJQKELy4wC7gSW5EtYa0pcxq1SrjIOISwLcfeSjkCb6mqi6zgr1XXcmjbN3sCYEMK2wMJm0l0LXBlC6AcMA7wQBpjZjY2kfwXY28w6m1lH4HvAloVmysx6AN0BH6DeGxgUQhgBnAx8GELoD+wCnG5m3YChwFbAtsDxwG6J8/3SzA5u5Hu6AYOBmwrNWw0qVRlDpgJ60sxmmNmP02SqXGUcfTbUzF4HJgInpslnjdB1nPI6bs0wyrdDCNMLSLcf0MviNV82MrMOIYQXgAZtWSGEOWY2GpgCLANmAisL+J6jzWwfYDlwYghhafSd94cQvorSHAD0MbMjo/cbAD2BvYA7oluTBWb2dCI/5zfxfWOBc0IIqxI/W1tTkjKODAwhLIxuwyab2WshhOfyfE+5y5gQwgRggpntC1wWnb8t0XWc8jpuTaX5RWJ7FZD8xuR6ugb0DyGsKPTEIYTxwHgAM7sSeKv5I4CoLSRPPg04LYTwRDKBmRV825DQD7g7+o/uAhxgZitDCA+24FzVqpRlvDB6XWRm9wP9gXyVZrnLOJnfp8zsFjPbMGU7bLXTdZzyOi5Kl6OoZv/EzHqa2Rrktl1MAU73N5ZpIG6WmX0req0DDiXT3oCZ/czMTm1FVh8DTrNMwy9m1svMOpBpbxketYlsDuyd70QhhG4hhLoQQh2ZW7eT21iFmaOYZWxmncysU7TdEdgfmBO9r5oyjtr8LNruR+ZBSVuqMHPoOi7sOi5mP83/JvPDPAcsSOw/Hdg9arCdC5wEedu7JkZpJwKnhhA+i/b3AT5qRR7HAW8Cs8xsDnADmWh7AvAeMBf4AzDVD2iuvWs1VKwy7go8a2YvA9PIPKGeEn1WTWU8jMwT3Flk2vSGtyJftULXcR41NfbczCYBQ4rd5UCqh8q47av1Mq6pSlNEpNI0jFJEJAVVmiIiKajSFBFJoVVrBHXp0iXU1dUVKSu1YcaMGUtWp1m9VcZtn8o4nVZVmnV1dUyfXshggrbDzFarZQFUxm2fyjgd3Z6LiKSgSlNEJAVVmiIiKajSFBFJQZWmiEgKqjSlrEaPHs3o0aMxM8yMqVOnMnXq1PwHilQJVZoiIim0qp+mSFpjx46tdBZEWkWRpohICoo0peTef//9BttXX301ALvuumtF8iTVafLkzCq699xzDwB/+ctfAPjkk0/yHrvGGpkY8IUXMksW9evXrxRZVKQpIpJGzUea8+dnhpBee+21ANkxtL///e8B2H777SuTMcm6++67G+zbYostKpATqQZ33XUXAA8+GC/F8/DDDwOwdGlmCSafHL1nz54AnHhivHrygAEDgPja9ruWm27KrMLrUaoiTRGRKlBTkeYbb7wBwHXXXZfdd+uttwLw6aef5qQ96KCDAHjooYey+7w9rXv37gB8+9vfLl1mJauxSFNtmauPc845B4Df/e53ACxfvhyIo0mAXr16AXDggQcCcMYZZwCw0047AbDWWms1ef7+/fsD8OabbwJw+eWXFy3vjVGkKSKSQlVHmqtWrQJg7ty5AOy///4ALFq0KO+xCxcuBGDvveOljz/7LLOCqEc5f//734H4qZsUl0f2zz//fHbflltumfMqbd8tt9wCwFdffQXAsGHDADj77LOzaXbccUcA1l577dTn33fffQEYMWIEAGuuuWbLM1sA1RYiIimo0hQRSaEqb88XL14MxA3Hl112WZNpN9xwQyC+9fZbeuf7k+bNm5eTVrfnpTFmzJgG+4YOHdri8/nEHsnO8pB7+/+DH/wA0IOmarL77rsDcN999wEwePBgAHbZZZeinH/rrbcuynkKpdpCRCSFqow0zz//fCDurOq8kfiaa67J7ttqq60AuPjii4HcqKO+TTbJLD53//33A9CuXVX++G3GggULGuwbOHBg6vN4hDl8+HCgYaSZ5NHtc889ByjirCTvIvjoo48C8bV6+OGHVyxPxaBIU0QkhYqHWsk2SG/v8kjQ2xq9E/rNN98MxIP6AUaNGgXE7ZTN2XnnnQFFH7XGo8f6EWZjk354NHrmmWcCaILjCrrxxhsB+PLLL4F4wMl6661XsTwVgyJNEZEUKh5p+kQbED9dc7179wbg3HPPBWCPPfYA4k6yhdhmm22y2+PGjWtxPqW8khFi/WGYPuGDd5JO8jbTxoZuSnl5hOmS12ItU6QpIpJCxSLNr7/+GoDf/OY3TabxdsojjzwyZ3/nzp2z2yNHjgRgypQpADz77LM5aU844YTstk/UIdWvsT6ezUWYTfFJbNMcI8UxadIkIG7DPOywwyqZnaJRpCkikkLFIk1/Mu59t6DhRBwdOnQAYJ111gHgv/7rv4D4ySjET1TrR6zetvWTn/ykmNmWFBqbaLixvptJXp7JNkkvS0WLtWHZsmVAPBrPp33z/XPmzGny2B49egDQvn37UmaxVRRpioikoEpTRCSFit2e+5x3vjYIxLOs+/DGvn37AnHXI+dhPsTDJ70bkjc6+xx+66+/frGzLgXy2beTD3XOOussILeJJck/T/JJOArht/c+X6du6cvPh7D6agq+btcOO+yQ91ifqf28884D4D//8z+B6rpdV6QpIpJCxTu3+9RuEM+8nI+vNgcNO8T7MLq20pG2lnm0l5ykwydUGT16NNB0xJmUb+VK71aUPL8PsZTy84e7vt7PBhtsADScwi35UPCll14CYObMmUB8h3DUUUcB8H//93/ZtJWOOhVpioikUPFIM42PP/4YaDyK6NatGxCvdy7Vw9s2Ib4T8LZLjww9TZqVKz3CTK4149FtmnZQKS5fq9ynhCuED7n09cAuvfRSAO644w4A+vTpk0174YUXFiWfLaVIU0QkhZqKNA855BAAXnnllQafXXTRRUDLVrOT0ko+wfZ2rLFjxwJxZNncBBs+eYe/Tpgwoclj/MmtVrusLT6QZbPNNgPg3XffzfncJxAsXRCzAAAG7klEQVSvBoo0RURSqIlI85133gEaH37l0edxxx1XzixJC/nTcm9z9D6cjU3Q4bwdtD5/Kp98eq4IszY988wzAPz0pz8FYPbs2QDsueeeABx99NGVyVgjFGmKiKSgSlNEJIWqvj1fuHAhAIMGDQLg888/B+LuRRB3MfJhmVIb/DbaO7n7q5dtYytOerckzXpUW1asWAHED2n//e9/A3DJJZdk0/h1/MUXXwBxGfvvRTWtK6RIU0QkhaqONH1oVf3uB8nZ2JNRp9Q+X5G0sQdD3l3Juxx5tKrVRauL3xH6ZDxvv/02AP/617+AeGKe5HXtQyN9Ah5/YFhNEaZTpCkikkJVRprTpk0D4Nhjj83Z7zO4H3zwwWXPk5SHt2EleWTpvL0r30QeUnorV64Ecoc2X3DBBUA8peMbb7wBwPLly4F41YbkRC7XX389EE8NV80UaYqIpFBVkaY/Ofuf//kfAJYuXZrz+UYbbQRAp06dypsxKbtkxNlY9CnVwScLvuqqqxp85sOd11prLQD69+8PxE/NDzrooHJksegUaYqIpFBVkeb48eOBhlNK+SD+Rx55BMidJkpEKmf33XcHcvvVzp8/H4Cf/exnQLzeuT+TqHWKNEVEUqiqSNNH9fgSGD4C5KSTTgKga9eulcmYiDRqyJAhOa+rA0WaIiIpqNIUEUmhqm7PfS49fxURqTaKNEVEUlClKSKSgipNEZEULITQ8oPNFgPzi5edmtA9hFA9S+OVmMq47VMZp9OqSlNEZHWj23MRkRRUaYqIpNBspWlmG5vZrOjfIjNbmHi/dqkyZWZnmtmr0b+RBaQ/0cwWR/l6zcxOyHdMnvPdZmaH5UmzkZlNMrOXo3we21z6alXBMu5sZvea2byozPrnSV/2Mk6k3dXMVhaavtpUsIwXmNkr0fe8UED6mriOm+3cHkL4COgbnfxiYFkI4bf1vtTItI2uyvdlhTCzvsCPgH7AN8DjZvZQCOEfeQ69PYQwysw2A+aY2QMhhCWJ87YLIXxTjDxGRgKzQgiDzWxTYJ6Z/bnI31FylSjjyO+AB0IIR0QXbocCjil3GWNm7YBfAZOLed5yqmAZA+wZQliaP1lW1V/HLbo9N7MeZjbXzG4HXgW2NLOlic+PNLObo+1No4hiuplNM7OBTZ030gd4PoTwZQjha+CvwOGF5i2EsAh4F+hmZpeb2a1m9izwRzNrZ2ajo3zMNrMTozyuYWbXR1HPZKBLIV8F+KpPnYAlwMpC81ntSlnGZtYZGBBC+CNACGFFCOHTQvNWxjIGGAXcSaZ825QSX8etUs3XcWvaNHsDY0II2wILm0l3LXBlCKEfMAzwQhhgZjc2kv4VYG/L3L51BL4HbFlopsysB9AdeCeRz0EhhBHAycCHIYT+wC7A6WbWDRgKbAVsCxwP7JY43y/NrLFFia4B+prZP4GXgZGh7XVFKFUZ/z9gcXQhzDSz8Wa2bqGZKlcZR8cNBm4qNG81qFRlDJkK6Ukzm2FmP06TqWq+jlsz9vztEML0AtLtB/TKRP8AbGRmHUIILwAN2jlCCHPMbDQwBVgGzKSwCO5oM9sHWA6cGEJYGn3n/SGEr6I0BwB9zOzI6P0GQE9gL+CO6NZkgZk9ncjP+U1838HANGBvYBvgUTPbIYSwrIC81oqSlDGZ37t+ZG6NZpC5Vf85cEme7yl3GY8FzgkhrEr8bG1NqcoYYGAIYWF0qz3ZzF4LITyX53uq/jpuTaX5RWJ7FZD8rWqf2DagfwhhRaEnDiGMB8YDmNmVwFsFHHZ7CGFUnnwacFoI4YlkAjMr+PY/4Xjg4uiv0utm9j6Z//SXWnCualWqMl4AvOcXq5ndQ+Y2OJ9yl3E/4O7oou0CHGBmK0MID7bgXNWqlNfxwuh1kZndD/QH8lWaVX8dF6XLUVSzf2JmPc1sDXLbIKcAp/sbyzzoaZaZfSt6rQMOJdOmhJn9zMxObUVWHwNOs0zjPmbWy8w6kGk3HR61iWxO5q9OPu8Bg6LzdAV6APkeVtWsYpZxCGEB8EF0CwaZ/8e50bFVU8YhhG4hhLoQQh0wETi5jVWYOYpZxmbWycw6Rdsdgf2BOdH7qiljWnAdF7Of5n+T+WGeIxNJuNOB3aMG27nASVEGm2sLmRilnQicGkL4LNrfB/ioFXkcB7wJzDKzOcANZKLtCWT+8+YCfwCm+gHNtIVcTKbtdTaZJ6tnhxA+aUXeakExy3gkcFf0/7cdcEW0v5rKeHVUrDLuCjxrZi+Tuf29L4QwJfqsmsr4YlJexzU1jNLMJgFDaq1bjxROZdz21XoZ11SlKSJSaRpGKSKSgipNEZEUVGmKiKSgSlNEJAVVmiIiKajSFBFJQZWmiEgK/x+2BJSMXnRtgwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_test_accuracy(show_example_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:    101, Training Accuracy:  70.3%\n",
      "Optimization Iteration:    201, Training Accuracy:  78.1%\n",
      "Optimization Iteration:    301, Training Accuracy:  81.2%\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=900) # We performed 100 iterations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
